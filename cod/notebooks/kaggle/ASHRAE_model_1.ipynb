{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_path = '/kaggle/input/ashrae-energy-prediction/train.csv'\nbuilding_train_path = '/kaggle/input/ashrae-energy-prediction/building_metadata.csv'\nweather_train_path = '/kaggle/input/ashrae-energy-prediction/weather_train.csv'\ntrain_df = pd.merge(pd.merge(pd.read_csv(train_path), pd.read_csv(building_train_path), on='building_id', how='left'), pd.read_csv(weather_train_path), on=['timestamp','site_id'], how='left')\ntrain_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.meter_reading = train_df.meter_reading.apply(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Function to reduce the DF size'''\n# source: https://www.kaggle.com/kernels/scriptcontent/3684066/download\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Variable Description'''\ndef description(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.iloc[0].values\n    summary['Second Value'] = df.iloc[1].values\n    summary['Third Value'] = df.iloc[2].values\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\ntrain_df[\"hour\"] = train_df[\"timestamp\"].dt.hour\ntrain_df[\"day\"] = train_df[\"timestamp\"].dt.day\ntrain_df[\"weekday\"] = train_df[\"timestamp\"].dt.weekday_name \ntrain_df[\"month\"] = train_df[\"timestamp\"].dt.month\ntrain_df['year'] = train_df['timestamp'].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_to_season = lambda month:(month%12 + 3)//3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['season'] = train_df.month.apply(month_to_season)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df.meter_reading","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_train = train_df.drop(columns=['meter_reading'])\nfeatures = ['hour','weekday', 'day', 'month', 'season'] # meter must be added, since not all features corr to meter_reading are the same. Same for site_id with less priority\n#features = ['meter', 'site_id', 'hour','weekday', 'day', 'month', 'season']\nx_train = train_df[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndef transform_features(dataset, is_regression=True):\n    features = []; categorical_features = []\n    num_of_columns = dataset.shape[1]\n \n    for i in range(0, num_of_columns):\n        column_name = dataset.columns[i]\n        column_type = dataset[column_name].dtypes\n\n        if i != num_of_columns - 1: #skip target\n            features.append(column_name)\n\n        if column_type == 'object':\n            le.fit(dataset[column_name])\n            feature_classes = list(le.classes_)\n            encoded_feature = le.transform(dataset[column_name])\n            dataset[column_name] = pd.DataFrame(encoded_feature)\n\n            if i != num_of_columns - 1: #skip target\n                categorical_features.append(column_name)\n\n    if is_regression == False and i == num_of_columns - 1:\n        num_of_classes = len(feature_classes)\n    else:\n        num_of_classes = 1\n\n    return dataset, features.copy(), categorical_features.copy(), num_of_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_regression = True\nx_train, features, categorical_features, num_of_classes = transform_features(x_train, is_regression=is_regression)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\n# Make validation set based on train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(x_train, y_train\n ,feature_name = features+categorical_features\n #, categorical_feature = categorical_features\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n 'task': 'train'\n , 'boosting_type': 'gbdt'\n , 'objective': 'regression' if is_regression == True else 'multiclass'\n , 'num_class': num_of_classes\n , 'metric': 'rmsle' if is_regression == True else 'multi_logloss'\n , 'min_data': 1\n , 'verbose': 1\n}\n \ngbm = lgb.train(params, lgb_train, num_boost_round=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install graphviz\n\nimport matplotlib.pyplot as plt\nax = lgb.plot_importance(gbm, max_num_features=10)\nplt.show()\n \nax = lgb.plot_tree(gbm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\ny_pred  =gbm.predict(x_val)\nscore = np.sqrt(mse(y_val, y_pred))# we use mse not msle since we already make y_val as np.log1p and so the y_pred\nprint('Val MSE = ', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ndel x_train\ndel x_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '/kaggle/input/ashrae-energy-prediction/test.csv'\nweather_test_path = '/kaggle/input/ashrae-energy-prediction/weather_test.csv'\n#!unzip -n test.csv.zip\n#!unzip -n weather_test.csv.zip\n\n# Test prep and features extraction\ntest_df = pd.merge(pd.merge(pd.read_csv(test_path), pd.read_csv(building_train_path), on='building_id', how='left'), pd.read_csv(weather_test_path), on=['timestamp','site_id'], how='left')\n\ntest_df = reduce_mem_usage(test_df)\n\ntest_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\ntest_df[\"hour\"] = test_df[\"timestamp\"].dt.hour\ntest_df[\"day\"] = test_df[\"timestamp\"].dt.day\ntest_df[\"weekday\"] = test_df[\"timestamp\"].dt.weekday_name \ntest_df[\"month\"] = test_df[\"timestamp\"].dt.month\ntest_df['year'] = test_df['timestamp'].dt.year\n\ntest_df['season'] = test_df.month.apply(month_to_season)\n\nx_test = test_df[features]\ndel test_df\nx_test, features, categorical_features, num_classes = transform_features(x_test, is_regression=is_regression)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score predicts to return the abs meter reading, while we predict log(p) + 1. So we must make exp(pred) - 1.\n\nThis is all done using np.expm1 (m1 = minus 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!unzip -n sample_submission.csv.zip\nsample = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/sample_submission.csv\")\nsample['meter_reading'] = np.expm1(gbm.predict(x_test))\nsample.to_csv('submission.csv', index=False)\nsample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}