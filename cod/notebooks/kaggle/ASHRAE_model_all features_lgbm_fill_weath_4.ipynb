{"cells":[{"metadata":{"id":"DHqXFHFHTk5m","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"Af87OrdlRcqh","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"kaggle = True  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"Ryh5dth8Q73g","colab_type":"code","colab":{}},"cell_type":"code","source":"if kaggle:\n  # This Python 3 environment comes with many helpful analytics libraries installed\n  # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n  # For example, here's several helpful packages to load in \n\n  import numpy as np # linear algebra\n  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n  # Input data files are available in the \"../input/\" directory.\n  # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n  import os\n  for dirname, _, filenames in os.walk('/kaggle/input'):\n      for filename in filenames:\n          print(os.path.join(dirname, filename))\n\n  # Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"wAIAN7MORaEs","colab_type":"code","outputId":"061ec7c5-cecb-48e2-e3d9-fd7bfda6629f","colab":{"base_uri":"https://localhost:8080/","height":386},"trusted":true},"cell_type":"code","source":"if not kaggle:\n  !pip install kaggle\n  from getpass import getpass\n  import os\n  user = 'ahmadelsallab'\n  key = '6b7ffe97ff5bc0656e325b746b72fa31'\n\n  if '.kaggle' not in os.listdir('/root'):\n      !mkdir ~/.kaggle\n  !touch /root/.kaggle/kaggle.json\n  !chmod 666 /root/.kaggle/kaggle.json\n  with open('/root/.kaggle/kaggle.json', 'w') as f:\n      f.write('{\"username\":\"%s\",\"key\":\"%s\"}' % (user, key))\n  !chmod 600 /root/.kaggle/kaggle.json\n\n  !kaggle competitions download -c ashrae-energy-prediction\n\n  !unzip -n train.csv.zip\n  !unzip -n weather_train.csv.zip\n  !unzip -n building_metadata.csv.zip","execution_count":null,"outputs":[]},{"metadata":{"id":"FKcSc5WWRI4A","colab_type":"code","outputId":"cf634b5d-4416-4b7d-93c8-984a04649dfb","colab":{"base_uri":"https://localhost:8080/","height":383},"trusted":true},"cell_type":"code","source":"# Original code from https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude\n\ndef fill_weather_dataset(weather_df):\n    \n    # Find Missing Dates\n    time_format = \"%Y-%m-%d %H:%M:%S\"\n    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n\n    missing_hours = []\n    for site_id in range(16):\n        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n        new_rows['site_id'] = site_id\n        weather_df = pd.concat([weather_df,new_rows])\n\n        weather_df = weather_df.reset_index(drop=True)           \n\n    # Add new Features\n    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n    \n    # Reset Index for Fast Update\n    weather_df = weather_df.set_index(['site_id','day','month'])\n\n    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n    weather_df.update(air_temperature_filler,overwrite=False)\n\n    # Step 1\n    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n    # Step 2\n    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n\n    weather_df.update(cloud_coverage_filler,overwrite=False)\n\n    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n    weather_df.update(due_temperature_filler,overwrite=False)\n\n    # Step 1\n    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n    # Step 2\n    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n\n    weather_df.update(sea_level_filler,overwrite=False)\n\n    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n    weather_df.update(wind_direction_filler,overwrite=False)\n\n    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n    weather_df.update(wind_speed_filler,overwrite=False)\n\n    # Step 1\n    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n    # Step 2\n    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n\n    weather_df.update(precip_depth_filler,overwrite=False)\n\n    weather_df = weather_df.reset_index()\n    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n        \n    return weather_df\n\n\ndef fill_with_po3(df):\n    return df.fillna(df.interpolate(method='polynomial', order=3))\n\ndef fill_with_lin(df):\n    return df.fillna(df.interpolate(method='linear'))\n\ndef fill_with_mix(df):\n    df = (df.fillna(df.interpolate(method='linear', limit_direction='both')) +\n               df.fillna(df.interpolate(method='polynomial', order=3, limit_direction='both'))\n              ) * 0.5\n    # workaround: fill last NANs with neighbour\n    assert df.count().min() >= len(df)-1 # only the first item is missing\n    return df.fillna(df.iloc[1])         # fill with second item\n\ndef fill_temps(weather):\n   #fill_with_lin(weather)\n    df = None\n    for col in ['air_temperature', 'dew_temperature']:\n        filled = fill_with_mix(weather.pivot(index='timestamp', columns='site_id', values=col))\n        filled = filled.sort_index().unstack().to_frame(col)\n        if df is None:\n            df = filled\n        else:\n            df[col] = filled[col]\n    return df   \n\ndef fill_missing_weather(weather):\n  #return fill_temps(weather)\n  wf = fill_temps(weather)\n  wf = wf.reset_index().merge(weather[['site_id', 'timestamp', 'cloud_coverage', 'precip_depth_1_hr', 'wind_direction', 'wind_speed']],\n                          how='left', on=['site_id', 'timestamp'])#.set_index(['site_id', 'timestamp'])\n  for col in ['cloud_coverage', 'precip_depth_1_hr', 'wind_direction', 'wind_speed']:\n      wf.loc[wf[col] < 0, col] = 0\n      wf.fillna(0, inplace=True)  \n  return wf\n\ndef load_data(data_path, building_path, weather_path):\n  weather = pd.read_csv(weather_path, parse_dates=['timestamp'])\n  weather = fill_missing_weather(weather)\n  return pd.merge(pd.merge(pd.read_csv(data_path, parse_dates=['timestamp']), pd.read_csv(building_path), on='building_id', how='left'), weather, on=['timestamp','site_id'], how='left')\n\ndef load_train_data():\n  if kaggle:\n    train_path = '/kaggle/input/ashrae-energy-prediction/train.csv'\n    building_train_path = '/kaggle/input/ashrae-energy-prediction/building_metadata.csv'\n    weather_train_path = '/kaggle/input/ashrae-energy-prediction/weather_train.csv'\n  else:\n    train_path = 'train.csv'\n    building_train_path = 'building_metadata.csv'\n    weather_train_path = 'weather_train.csv'\n\n  #return pd.merge(pd.merge(pd.read_csv(train_path), pd.read_csv(building_train_path), on='building_id', how='left'), pd.read_csv(weather_train_path), on=['timestamp','site_id'], how='left')\n  return load_data(train_path, building_train_path, weather_train_path)\n'''\ndef load_train_data():\n  if kaggle:\n    train_path = '/kaggle/input/ashrae-energy-prediction/train.csv'\n    building_train_path = '/kaggle/input/ashrae-energy-prediction/building_metadata.csv'\n    weather_train_path = '/kaggle/input/ashrae-energy-prediction/weather_train.csv'\n  else:\n    train_path = 'train.csv'\n    building_train_path = 'building_metadata.csv'\n    weather_train_path = 'weather_train.csv'\n\n  return pd.merge(pd.merge(pd.read_csv(train_path), pd.read_csv(building_train_path), on='building_id', how='left'), pd.read_csv(weather_train_path), on=['timestamp','site_id'], how='left')\n'''\ntrain_df = load_train_data()\ntrain_df.head()\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kDGJlP4dQ73z","colab_type":"code","colab":{}},"cell_type":"code","source":"\ntrain_df.meter_reading = train_df.meter_reading.apply(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"id":"wA_d-QKQAt_I","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_df.square_feet = train_df.square_feet.apply(np.log1p)\n#train_df['square_feet'] =  np.log1p(train_df['square_feet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"5r7lptAxQ735","colab_type":"code","colab":{}},"cell_type":"code","source":"'''\nreduce_mem from 1.08\n# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n'''\n'''Function to reduce the DF size'''\n# source: https://www.kaggle.com/kernels/scriptcontent/3684066/download\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object and col_type != '<M8[ns]':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"LCleRh7tQ74D","colab_type":"code","colab":{}},"cell_type":"code","source":"'''Variable Description'''\ndef description(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.iloc[0].values\n    summary['Second Value'] = df.iloc[1].values\n    summary['Third Value'] = df.iloc[2].values\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"id":"mztwu1hjO9qm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"230a6139-177f-4223-a03c-8a567e59ca17","trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"P0d-vamDQ74I","colab_type":"code","outputId":"a9dfbc56-8abc-483c-85cc-710bd7ab0970","colab":{"base_uri":"https://localhost:8080/","height":521}},"cell_type":"code","source":"description(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ZHF6BtqQQ74M","colab_type":"code","colab":{}},"cell_type":"code","source":"train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\ntrain_df[\"hour\"] = train_df[\"timestamp\"].dt.hour\ntrain_df[\"day\"] = train_df[\"timestamp\"].dt.day\ntrain_df[\"weekday\"] = train_df[\"timestamp\"].dt.weekday_name \ntrain_df[\"month\"] = train_df[\"timestamp\"].dt.month\ntrain_df['year'] = train_df['timestamp'].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"oGYeiD7CQ74R","colab_type":"code","outputId":"38c50338-3268-404c-8802-4c6a813402eb","colab":{"base_uri":"https://localhost:8080/","height":299}},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8rsPg6InQ74X","colab_type":"code","colab":{}},"cell_type":"code","source":"month_to_season = lambda month:(month%12 + 3)//3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"OBMra8EXQ74b","colab_type":"code","colab":{}},"cell_type":"code","source":"train_df['season'] = train_df.month.apply(month_to_season)","execution_count":null,"outputs":[]},{"metadata":{"id":"t1XTAnzwIidP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":698},"outputId":"3c498744-8202-4a0a-ff9a-f0e6649ec536","trusted":true},"cell_type":"code","source":"description(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9FZVH-6LQ74h","colab_type":"code","colab":{}},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"K_i_o9tOQ74m","colab_type":"code","colab":{}},"cell_type":"code","source":"y_train = train_df.meter_reading","execution_count":null,"outputs":[]},{"metadata":{"id":"8R87s-_a5mzx","colab_type":"text"},"cell_type":"markdown","source":"Note that: LGBM will ignore missing values in the features vector.\nhttps://www.kaggle.com/c/home-credit-default-risk/discussion/57918\n\nsemissing=false, which disables handling for missing values. You can also use the zeroas_missing option to change behavior.\n\nSo we need to handle missing values for better performance. Especially for __temprature values__"},{"metadata":{"trusted":true,"id":"5bcrFReGQ74s","colab_type":"code","colab":{}},"cell_type":"code","source":"features = ['meter',\n            'site_id',\n            'building_id',\n            'primary_use',\n            'square_feet',\n            #'year_built',\n            #'floor_count',\n            'air_temperature',\n            'dew_temperature',\n            #'cloud_coverage',\n            #'wind_direction',\n            #'wind_speed',\n            #'precip_depth_1_hr',\n            'hour',\n            #'weekday',\n            #'day',\n            'month',\n            'season'\n            ] # meter must be added, since not all features corr to meter_reading are the same. Same for site_id with less priority\ndef select_features(df):    \n    return df[features] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2yqbU8afQ74w","colab_type":"code","colab":{}},"cell_type":"code","source":"\nx_train = select_features(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"4qFycIs7Q741","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndef prep_features(dataset):\n    features = []; categorical_features = []\n    num_of_columns = dataset.shape[1]\n \n    for i in range(0, num_of_columns):\n        column_name = dataset.columns[i]\n        column_type = dataset[column_name].dtypes\n        '''\n        if i != num_of_columns - 1: #skip target\n            features.append(column_name)\n        '''\n        features.append(column_name)\n        if column_type == 'object' or column_type.name == 'category':\n            le.fit(dataset[column_name])\n            feature_classes = list(le.classes_)\n            encoded_feature = le.transform(dataset[column_name])\n            dataset[column_name] = pd.DataFrame(encoded_feature)\n            '''\n            if i != num_of_columns - 1: #skip target\n                categorical_features.append(column_name)\n            '''\n            categorical_features.append(column_name)\n    '''\n    if is_regression == False and i == num_of_columns - 1:\n        num_of_classes = len(feature_classes)\n    else:\n        num_of_classes = 1\n    '''\n    return dataset, features, categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WVAK6OMuQ746","colab_type":"code","colab":{}},"cell_type":"code","source":"\nx_train, features, categorical_features = prep_features(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.primary_use.dtype.name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"eL3wDPOcQ74_","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\n# Make validation set based on train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the type of some categorical features could be int, but we know they are categories, like site_id or meter.\nFOr that, we manually set the cat_features"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['meter', 'site_id', 'building_id', 'primary_use', 'month', 'season']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"iuaZ-KCuQ75E","colab_type":"code","colab":{}},"cell_type":"code","source":"'''\nlgb_train = lgb.Dataset(x_train, y_train\n ,feature_name = features#+categorical_features\n , categorical_feature = categorical_features\n)\n\nparams = {\n 'task': 'train'\n , 'boosting_type': 'gbdt'\n , 'objective': 'regression'# if is_regression == True else 'multiclass'\n , 'num_class': 1\n , 'metric': 'rmsle'# if is_regression == True else 'multi_logloss'\n , 'min_data': 1\n , 'verbose': 1\n}\n \ngbm = lgb.train(params, lgb_train, num_boost_round=50)\n\n'''\n\n\nd_training = lgb.Dataset(x_train, label=y_train,categorical_feature=categorical_features, free_raw_data=False)\nd_val = lgb.Dataset(x_val, label=y_val,categorical_feature=categorical_features, free_raw_data=False)\n\nparams = {\n    \"objective\": \"regression\",\n    \"boosting\": \"gbdt\",\n    \"num_leaves\": 1280,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.85,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\",\n}\n\nmodel = lgb.train(params, train_set=d_training, num_boost_round=1000, valid_sets=[d_training,d_val], verbose_eval=25, early_stopping_rounds=50)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The warning above is not because of something wrong.\nSee: https://github.com/Microsoft/LightGBM/issues/1408\n@JYLFamily This warning is raised because you're passing categorical features twice: when constuct Dataset and when call train.\n\nThe correct way is to pass them only while constructing Dataset via categorical_feature argument.\n\n"},{"metadata":{"trusted":true,"id":"XTR_kXsVQ75Q","colab_type":"code","outputId":"fe0bc1f8-412c-42e4-c4db-1039fa87779a","colab":{"base_uri":"https://localhost:8080/","height":394}},"cell_type":"code","source":"!pip install graphviz\n\nimport matplotlib.pyplot as plt\nax = lgb.plot_importance(model, max_num_features=10)\nplt.show()\n \n#ax = lgb.plot_tree(model)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kjdokTg6Q75Z","colab_type":"code","outputId":"f40e6a0e-07bf-4887-f601-0028698ee944","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\ny_pred = model.predict(x_val)\nscore = np.sqrt(mse(y_val, y_pred))# we use mse not msle since we already make y_val as np.log1p and so the y_pred\nprint('Val MSE = ', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PLijTPVsQ75h","colab_type":"code","colab":{}},"cell_type":"code","source":"del train_df, x_train, y_train, x_val, y_val, d_training, d_val\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"zQ2E7dkrSXXE","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def load_test_data():\n  if kaggle:\n    test_path = '/kaggle/input/ashrae-energy-prediction/test.csv'\n    weather_test_path = '/kaggle/input/ashrae-energy-prediction/weather_test.csv'\n    building_train_path = '/kaggle/input/ashrae-energy-prediction/building_metadata.csv'\n  else:\n    test_path = 'test.csv'\n    weather_test_path = 'weather_test.csv'    \n    building_train_path = 'building_metadata.csv'\n    \n  \n  return pd.merge(pd.merge(pd.read_csv(test_path), pd.read_csv(building_train_path), on='building_id', how='left'), pd.read_csv(weather_test_path), on=['timestamp','site_id'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ulx7HKFgQ75n","colab_type":"code","outputId":"69b7f83a-ea2d-4229-90f0-138e7414a9d9","colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":"\nif not kaggle:\n  !unzip -n test.csv.zip\n  !unzip -n weather_test.csv.zip\n  \ntest_df = load_test_data()\n# Test prep and features extraction\n\n\ntest_df = reduce_mem_usage(test_df)\n\ntest_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\ntest_df[\"hour\"] = test_df[\"timestamp\"].dt.hour\ntest_df[\"day\"] = test_df[\"timestamp\"].dt.day\ntest_df[\"weekday\"] = test_df[\"timestamp\"].dt.weekday_name \ntest_df[\"month\"] = test_df[\"timestamp\"].dt.month\ntest_df['year'] = test_df['timestamp'].dt.year\n\ntest_df['season'] = test_df.month.apply(month_to_season)\n#test_df = reduce_mem_usage(test_df)\n\nx_test = select_features(test_df)\ndel test_df\nx_test, features, categorical_features = prep_features(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"lYLiT0qUQ75s","colab_type":"text"},"cell_type":"markdown","source":"The score predicts to return the abs meter reading, while we predict log(p) + 1. So we must make exp(pred) - 1.\n\nThis is all done using np.expm1 (m1 = minus 1)"},{"metadata":{"id":"CKaTqe5ZxRkh","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"preds = np.expm1(model.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"xa4QXiPVQ75t","colab_type":"code","outputId":"7a02a3f9-17d8-4611-887c-468ffc6388fa","colab":{"base_uri":"https://localhost:8080/","height":230}},"cell_type":"code","source":"#!unzip -n sample_submission.csv.zip\nif kaggle:\n  sample = pd.read_csv(\"/kaggle/input/ashrae-energy-prediction/sample_submission.csv\")\nelse:\n  !unzip -n sample_submission.csv.zip\n  sample = pd.read_csv(\"sample_submission.csv\")\nsample['meter_reading'] = preds #np.expm1(gbm.predict(x_test))\nsample.to_csv('submission.csv', index=False)\nsample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"ASHRAE_model_all features_lgbm.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}